---
layout:     post                    # 使用的布局（不需要改）
title:      机器学习相关算法               # 标题 
subtitle:   机器学习占个坑 #副标题
date:       2019-09-01              # 时间
author:     SH                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - ML
    - DL
---

## Hey
这是我开始机器学习的第一篇博客。我看的第一篇论文是吴韵怡师姐的硕士毕业论文。
以下部分关于机器学习的介绍摘自她的毕业论文：
> 机器学习（Machine Learning）是人工智能的一个分支，是一门涉及概率论、统
计学、算法复杂度理论等多学科知识的多领域交叉学科；它使用复杂的算法使计算
机能够从数据中学习，从而使计算机学习新的知识或技能，并做出合理预测[38]。机
器学习的主要算法是从聚类分析和模式识别的研究演变而来的，包括人工神经网
络，决策树，支持向量机和贝叶斯分类器等[39]。除了聚类分析和模式识别外，这些
算法还与数据挖掘有很大关联[40]。

> 1957 年，Rosenblatt 提出了一个模拟神经元结构的感知器模型（Perceptron），
并用作二元分类器[41]。Widrow 和 Hoff 首次使用了 Delta 学习规则来训练感知器并
为线性分类器奠定基础[42]。1967 年，Cover 和 Hart 提出了最近邻算法，这个算法
根据空间特征对样本点进行分类[43]。1986 年，Quilan 提出了决策树算法[44]。1995
年，Cortes 等人提出了支持向量机算法，其关键思想是找到一个边界，根据最大距
离划分为两个类别；除线性分类外，支持向量机还可以应用于高维非线性分类[45]。
2001 年，Breiman 提出了随机森林算法[46]，这是一个具有多个决策树的分类器，每
个树输出它们各自的预测类别，然后每个树投票以确定分类器的最终输出类别[47]；
随机森林算法广泛用于解决多类分类问题。支持向量机和随机森林都是基于统计
数据的算法；因此，它们在结构化和密集型数据集中表现较佳。

> 1986 年，Hinton 等人提出了使用 sigmoid 激活函数的多层感知机（Multilayer
Perceptron），并运用反向传播算法来执行非线性映射，该方法有效地使用多层感知
机来解决非线性分类和训练问题[48]。如图 1-6，多层感知机模型包含了输入层、隐
藏层和输出层，其中层与层之间由参数连接，每一个神经元都由前一层所有神经元
传递的信息来计算，并执行 sigmoid 激活函数映射之后的作为结果并继续输入到后
一层，直到最终抵达输出层[49]。

> [38] T Mitchell, B Buchanan, G Dejong, et al. Machine Learning[J]. 1990, 4(1): 417-433.

> [39] Bishop C M. Pattern Recognition and Machine Learning (Information Science and 
Statistics)[M]. City: Springer-Verlag, 2006.

> [40] Fürnkranz J, Gamberger D, Lavrač N. Machine Learning and Data Mining[J]. 
Computer Study, 2010, 42(2): 110-114.

> [41] Rosenblatt F. The perceptron: A probabilistic model for information storage and 
organization in the brain[M]. City: MIT Press, 1988.

> [42] Widrow B, Hoff M E. Adaptive Switching Circuits[C]. Ire Wescon Conv Rec, 1966.

> [43] Cover T, Hart P. Nearest neighbor pattern classification[J]. IEEE Transinftheory, 
2002, 13(1): 21-27.

> [44] Quinlan J R. Induction of decision trees[J]. Machine Learning, 1986, 1(1): 81-106.

> [45] Cortes C, Vapnik V J M L. Support-vector networks[J]. 1995, 20(3): 273-297.

> [46] Breiman L. Random Forests[J]. Machine Learning, 2001, 45(1): 5-32.

> [47] Tin Kam H. Random decision forests[C]. Proceedings of 3rd International 
Conference on Document Analysis and Recognition, 1995: 278-282 vol.271.

> [48] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back propagating errors[J]. Readings in Cognitive Science, 1986, 323(6088): 399-421.

> [49] Mirjalili S, Mirjalili S M, Lewis A. Let a biogeography-based optimizer train your 
Multi-Layer Perceptron[J]. Information Sciences, 2014, 269: 188-209.
